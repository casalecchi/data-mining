{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separação de arquivos para treino e teste do modelo\n",
    "\n",
    "* Rodar em menos tempo - apenas dois arquivos para o treino ~ aproximadamente 1 hora.\n",
    "* Um arquivo para o teste e ter métricas.\n",
    "* Um arquivo de resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [\n",
    "    \"../data/bus/final/2024-05-17/2024-05-17_09.json\",\n",
    "    \"../data/bus/final/2024-05-17/2024-05-17_10.json\",\n",
    "]\n",
    "\n",
    "test_files = [\n",
    "    \"../data/bus/final/2024-05-17/teste-2024-05-17_15.json\",\n",
    "]\n",
    "\n",
    "answer_files = [\n",
    "    \"../data/bus/teste/2024-05-15/resposta-2024-05-15_08.json\"\n",
    "]\n",
    "\n",
    "train_first = pd.read_json(train_files[0], encoding='latin-1')\n",
    "train_second = pd.read_json(train_files[1], encoding='latin-1')\n",
    "\n",
    "train = pd.concat([train_first, train_second])\n",
    "test = pd.read_json(test_files[0], encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento das colunas e linhas de ônibus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "    '483', '864', '639', '3', '309', '774', '629', '371', '397', '100', '838', \n",
    "    '315', '624', '388', '918', '665', '328', '497', '878', '355', '138', '606', \n",
    "    '457', '550', '803', '917', '638', '2336', '399', '298', '867', '553', '565', \n",
    "    '422', '756', '186012003', '292', '554', '634', '232', '415', '2803', '324', \n",
    "    '852', '557', '759', '343', '779', '905', '108'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['latitude'] = train['latitude'].str.replace(',', '.').astype(float)\n",
    "train['longitude'] = train['longitude'].str.replace(',', '.').astype(float)\n",
    "train['linha'] = train['linha'].astype(str)\n",
    "train = train[train['linha'].isin(lines)]\n",
    "test['linha'] = test['linha'].astype(str)\n",
    "test = test[test['linha'].isin(lines)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_two = train.groupby(['ordem', 'linha']).tail(2).reset_index(drop=True)\n",
    "size = last_two.groupby(['ordem', 'linha']).size()\n",
    "to_duplicate = size[size == 1].index\n",
    "duplicats = last_two.set_index(['ordem', 'linha']).loc[to_duplicate].reset_index()\n",
    "last_two = pd.concat([last_two, duplicats]).sort_values(['ordem', 'linha'])\n",
    "# Deixar apenas algumas colunas\n",
    "last_two = last_two[['ordem','linha','latitude','longitude','datahoraservidor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntar DataFrame de teste com o de treino modificado\n",
    "join_df = pd.merge(test, last_two, on=['ordem','linha'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conectar ao Banco de Dados PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_url = \"postgresql://postgres:camila@localhost:5432/postgres\"\n",
    "engine = create_engine(database_url, client_encoding='latin-1')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(connection, linha, lat1, lon1, lat2, lon2, last_date, prediction_date):\n",
    "    query = \"\"\"\n",
    "    WITH initial_similar_points AS (\n",
    "        SELECT time_ranking,\n",
    "               ordem,\n",
    "               linha,\n",
    "               x,\n",
    "               y,\n",
    "               datahoraservidor\n",
    "        FROM vw_buses_order\n",
    "        WHERE linha = :linha\n",
    "        AND x = width_bucket(:lon1, -43.726090, -42.951470, 1587)\n",
    "        AND y = width_bucket(:lat1, -23.170790, -22.546410, 1389)\n",
    "        AND (\n",
    "                (datahoraservidor >= TO_TIMESTAMP(:last_date) - interval '7 day' - interval '2 hour'  \n",
    "                AND datahoraservidor < TO_TIMESTAMP(:last_date) - interval '7 day' + interval '2 hour') \n",
    "                OR \n",
    "                (datahoraservidor >= TO_TIMESTAMP(:last_date) - interval '14 day' - interval '2 hour'  \n",
    "                AND datahoraservidor < TO_TIMESTAMP(:last_date) - interval '14 day' + interval '2 hour')\n",
    "                OR \n",
    "                (datahoraservidor >= TO_TIMESTAMP(:last_date) - interval '21 day' - interval '2 hour'  \n",
    "                AND datahoraservidor < TO_TIMESTAMP(:last_date) - interval '21 day' + interval '2 hour')\n",
    "            )\n",
    "        AND time_ranking > 1\n",
    "        LIMIT 10\n",
    "    ), anterior_points AS (\n",
    "        SELECT DISTINCT ON (time_ranking, ordem, linha) \n",
    "            time_ranking,\n",
    "            ordem,\n",
    "            linha,\n",
    "            x,\n",
    "            y,\n",
    "            datahoraservidor\n",
    "        FROM vw_buses_order\n",
    "        WHERE (ordem, linha, time_ranking) IN (\n",
    "            SELECT ordem, linha, time_ranking - 1\n",
    "            FROM initial_similar_points\n",
    "            )\n",
    "    ), direction_points AS (\n",
    "         SELECT \n",
    "            sp.ordem,\n",
    "            sp.datahoraservidor\n",
    "        FROM initial_similar_points sp\n",
    "        INNER JOIN anterior_points ap\n",
    "            ON sp.ordem = ap.ordem\n",
    "            AND sp.linha = ap.linha\n",
    "            AND sp.time_ranking = ap.time_ranking + 1\n",
    "        WHERE ((ap.x - sp.x) * (:lon2 - :lon1) + (ap.y - sp.y) * (:lat2 - :lat1)) >= 0\n",
    "    ), first_future_points AS (\n",
    "        SELECT DISTINCT ON (vo.ordem, vo.datahoraservidor)\n",
    "            vo.x,\n",
    "            vo.y,\n",
    "            vo.ordem,\n",
    "            vo.datahoraservidor              \n",
    "        FROM (\n",
    "                SELECT \n",
    "                          ordem,\n",
    "                          linha,\n",
    "                          x,\n",
    "                          y,\n",
    "                          datahoraservidor\n",
    "                FROM vw_buses_order\n",
    "                WHERE linha = :linha\n",
    "                AND ordem IN (SELECT DISTINCT ordem FROM direction_points)\n",
    "             ) vo\n",
    "        INNER JOIN direction_points dp\n",
    "            ON vo.ordem = dp.ordem\n",
    "            AND vo.datahoraservidor > dp.datahoraservidor\n",
    "            AND vo.datahoraservidor < dp.datahoraservidor + interval '1 hour' + interval '20 minutes'\n",
    "        WHERE vo.datahoraservidor > dp.datahoraservidor + (TO_TIMESTAMP(:prediction_date) - TO_TIMESTAMP(:last_date) - interval '2 minutes')\n",
    "        AND vo.datahoraservidor < dp.datahoraservidor + (TO_TIMESTAMP(:prediction_date) - TO_TIMESTAMP(:last_date) + interval '2 minutes')\n",
    "    ), selected_future_points AS (\n",
    "        SELECT x,y\n",
    "        FROM first_future_points\n",
    "    )\n",
    "    SELECT \n",
    "        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY x)) AS median_x,\n",
    "        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY y)) AS median_y\n",
    "    FROM selected_future_points;\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'linha': linha,\n",
    "        'lat1': lat1,\n",
    "        'lon1': lon1,\n",
    "        'lat2': lat2,\n",
    "        'lon2': lon2,\n",
    "        'last_date': str(last_date),\n",
    "        'prediction_date': str(prediction_date)\n",
    "    }\n",
    "\n",
    "    result = connection.execute(text(query), params)\n",
    "    row = result.fetchone()\n",
    "        \n",
    "    return row[0], row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/rr_qb8ln101dbc1k0yyw601r0000gn/T/ipykernel_36927/1536774100.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(0, len(join_df)- 1, 2)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0bc0f4d99b486cb0c926791139f75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148879 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "median_x_list = []\n",
    "median_y_list = []\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    for i in tqdm_notebook(range(0, len(join_df)- 1, 2)):\n",
    "        row1 = join_df.iloc[i + 1]\n",
    "        row2 = join_df.iloc[i]\n",
    "        median_x, median_y = execute_query(\n",
    "            connection,\n",
    "            row1['linha'], \n",
    "            row1['latitude'], \n",
    "            row1['longitude'], \n",
    "            row2['latitude'], \n",
    "            row2['longitude'], \n",
    "            row1['datahoraservidor']/1000, # Convert to seconds - Last Date\n",
    "            row1['datahora']/1000 # Convert to seconds - Prediction Date\n",
    "        )\n",
    "\n",
    "        median_x_list.extend([median_x, median_x])\n",
    "        median_y_list.extend([median_y, median_y])\n",
    "\n",
    "join_df['median_x'] = median_x_list\n",
    "join_df['median_y'] = median_y_list\n",
    "prediction = join_df[['id','latitude', 'longitude', 'median_y','median_x']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0.0\n",
       "latitude       0.0\n",
       "longitude      0.0\n",
       "median_y     100.0\n",
       "median_x     100.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prediction.isnull().sum()/len(prediction)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento de dados dos resultados\n",
    "\n",
    "* Tirar duplicatas\n",
    "* Deixar valores novos para valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_indices = prediction.index[prediction.index % 2 == 0]\n",
    "rows_with_nulls = prediction.loc[even_indices].isnull().any(axis=1)\n",
    "prediction = prediction.drop(even_indices[rows_with_nulls])\n",
    "prediction['median_x'] = prediction['median_x'].fillna(prediction['longitude'])\n",
    "prediction['median_y'] = prediction['median_y'].fillna(prediction['latitude'])\n",
    "prediction = prediction[['id','median_y','median_x']]\n",
    "prediction.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_width_bucket_x(bucket_index):\n",
    "    min_value = -43.726090\n",
    "    max_value = -42.951470\n",
    "    num_buckets = 1587\n",
    "\n",
    "    if bucket_index < 1:\n",
    "        return min_value\n",
    "    elif bucket_index > num_buckets:\n",
    "        return max_value\n",
    "    \n",
    "    bucket_size = (max_value - min_value) / num_buckets\n",
    "    value = min_value + (bucket_index - 1) * bucket_size + bucket_size / 2\n",
    "    \n",
    "    return value\n",
    "    \n",
    "def inverse_width_bucket_y(bucket_index):\n",
    "    min_value = -23.170790\n",
    "    max_value = -22.546410\n",
    "    num_buckets = 1389\n",
    "\n",
    "    if bucket_index < 1:\n",
    "        return min_value\n",
    "    elif bucket_index > num_buckets:\n",
    "        return max_value\n",
    "    \n",
    "    bucket_size = (max_value - min_value) / num_buckets\n",
    "    value = min_value + (bucket_index - 1) * bucket_size + bucket_size / 2\n",
    "    \n",
    "    return value\n",
    "\n",
    "prediction['median_x'] = prediction['median_x'].apply(inverse_width_bucket_x)\n",
    "prediction['median_y'] = prediction['median_y'].apply(inverse_width_bucket_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction['id'] = prediction['id'].astype('Int64')\n",
    "previsoes = prediction.values.tolist()\n",
    "\n",
    "match = re.search(r'teste-(\\d{4}-\\d{2}-\\d{2})_(\\d{2})', test_files[0])\n",
    "date_part = match.group(1)\n",
    "hour_part = match.group(2)\n",
    "datahora = f\"{date_part} {hour_part}:00:00\"\n",
    "\n",
    "output = {\n",
    "    \"aluno\": \"Felipe Vilela Magalhães Casalecchi\",\n",
    "    \"datahora\": datahora,\n",
    "    \"previsoes\": [[str(item) if isinstance(item, pd.Int64Dtype) else item for item in row] for row in previsoes],\n",
    "    \"senha\": \"dataMining\"\n",
    "}\n",
    "\n",
    "output_json = json.dumps(output, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON salvo em 24-05-17_15_answer.json\n"
     ]
    }
   ],
   "source": [
    "output_filename = test_files[0][37:48] + \"_answer.json\"\n",
    "with open(output_filename, \"w\") as json_file:\n",
    "    json_file.write(output_json)\n",
    "\n",
    "print(f\"JSON salvo em {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandar resposta para Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST bem-sucedido!\n",
      "Resposta do servidor:\n",
      "{'msg': 'Problemas!', 'arquivo teste': 'teste-2024-05-17_15.json', 'rmse': 53263.207945918555, 'ids não encontrados': 0, 'ids testados': 148879, 'total na tabela': 162697}\n"
     ]
    }
   ],
   "source": [
    "url = 'https://barra.cos.ufrj.br:443/rest/rpc/avalia'\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "response = requests.post(url, headers=headers, data=output_json)\n",
    "if response.status_code == 200:\n",
    "    print(\"POST bem-sucedido!\")\n",
    "    print(\"Resposta do servidor:\")\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Falha no POST: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
